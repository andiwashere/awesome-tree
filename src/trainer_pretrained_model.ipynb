{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Collecting nlp\n",
      "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill in /home/marisa/anaconda3/lib/python3.7/site-packages (from nlp) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/marisa/anaconda3/lib/python3.7/site-packages (from nlp) (4.42.1)\n",
      "Requirement already satisfied: pandas in /home/marisa/anaconda3/lib/python3.7/site-packages (from nlp) (1.0.1)\n",
      "Requirement already satisfied: xxhash in /home/marisa/anaconda3/lib/python3.7/site-packages (from nlp) (2.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/marisa/anaconda3/lib/python3.7/site-packages (from nlp) (2.22.0)\n",
      "Requirement already satisfied: numpy in /home/marisa/anaconda3/lib/python3.7/site-packages (from nlp) (1.18.1)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /home/marisa/anaconda3/lib/python3.7/site-packages (from nlp) (1.0.1)\n",
      "Requirement already satisfied: filelock in /home/marisa/anaconda3/lib/python3.7/site-packages (from nlp) (3.0.12)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/marisa/anaconda3/lib/python3.7/site-packages (from pandas->nlp) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/marisa/anaconda3/lib/python3.7/site-packages (from pandas->nlp) (2.8.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/marisa/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/marisa/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/marisa/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/marisa/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (2019.11.28)\n",
      "Requirement already satisfied: six>=1.5 in /home/marisa/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->nlp) (1.14.0)\n",
      "Installing collected packages: nlp\n",
      "Successfully installed nlp-0.4.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!pip install nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.tripadvisor import TripAdvisorDataset\n",
    "\n",
    "dataset = TripAdvisorDataset(\n",
    "        text_processor='word2vec', \n",
    "        text_processor_filters=['lowercase', 'stopwordsfilter'],\n",
    "        embedding_path='../data/embeddings/word2vec/glove.6B.50d.txt',\n",
    "        data_path='../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "annotators = []\n",
    "\n",
    "for datapoint in dataset:\n",
    "    annotators.append(datapoint['annotator'])\n",
    "    texts.append(datapoint['text'])\n",
    "    labels.append(datapoint['label'].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do train, validation and test split. (0.6, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Test set is 20%, validation set is 25% of the rest (0.25 x 0.8 = 0.2)\n",
    "# which translates to 20%, training set is the 60%\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizerFast\n",
    "\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_encoded = tokenizer(x_train, truncation=True, padding=True)\n",
    "x_val_encoded = tokenizer(x_val, truncation=True, padding=True)\n",
    "x_test_encoded = tokenizer(x_test, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a class that will allow us to get the data ready for transformers.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# https://huggingface.co/transformers/custom_datasets.html\n",
    "\n",
    "class CurrentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.column_names = ['input_ids', 'attention_mask', 'labels']\n",
    "        self.set_format = None\n",
    "        self.format = None\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = dataset.data['train']\n",
    "val_dataset = dataset.data['validation']\n",
    "test_dataset = dataset.data['test']\n",
    "\n",
    "print('train datapoint:\\n', train_dataset[0])\n",
    "print('\\n\\nval datapoint:\\n', val_dataset[0])\n",
    "print('\\n\\ntest datapoint:\\n', test_dataset[0])\n",
    "print('\\n dataset.data.column_names: ', dataset.annotators)\n",
    "\n",
    "train_dataset = CurrentDataset(x_train_encoded, y_train)\n",
    "val_dataset = CurrentDataset(x_val_encoded, y_val)\n",
    "test_dataset = CurrentDataset(x_test_encoded, y_test)\n",
    "\n",
    "print('\\n\\ntrain datapoint:\\n', train_dataset[0])\n",
    "print('\\n\\nval datapoint:\\n', val_dataset[0])\n",
    "print('\\n\\ntest datapoint:\\n', test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is just to inform the terminal of the stage of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('COMENCING TRAINING & FINE-TUNING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b03560a43cc4f5d830ec272aa524c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=4556.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f66041bf4af4b23b76756c5b0ab9014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2071.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.06 MiB, post-processed: Unknown sizetotal: 207.28 MiB) to /home/marisa/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1668ebed5b3c47de98a5558b641b4434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=84125825.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Dataset imdb downloaded and prepared to /home/marisa/.cache/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743. Subsequent calls will reuse this data.\n",
      "['label', 'text']\n"
     ]
    }
   ],
   "source": [
    "from nlp import load_dataset\n",
    "train = load_dataset(\"imdb\", split=\"train\")\n",
    "print(train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a503676c5e5f4a6aa15ea91ea9cdfcea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = train.map(lambda batch: tokenizer(batch[\"text\"], truncation=True, padding=True), batched=True)\n",
    "train.rename_column_(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([]),\n",
       " 'input_ids': torch.Size([512]),\n",
       " 'attention_mask': torch.Size([512])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "{key: val.shape for key, val in train[0].items()}\n",
    "{'labels': torch.Size([]), 'input_ids': torch.Size([512]), 'attention_mask': torch.Size([512])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Test set is 20%, validation set is 25% of the rest (0.25 x 0.8 = 0.2)\n",
    "# which translates to 20%, training set is the 60%\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(train, test_size=0.2, random_state=1)\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.25, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set the model, the training arguments and we instantiate a Trainer. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# https://huggingface.co/transformers/custom_datasets.html\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=2,   # batch size per device during training\n",
    "    per_device_eval_batch_size=2,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=train_dataset           # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
